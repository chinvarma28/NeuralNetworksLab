{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFf14T7lDR4YOqakBUedgY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mysterious-27/21AI72-NN-and-DL/blob/main/p1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Taking our own input\n"
      ],
      "metadata": {
        "id": "JE_ihAd9tO8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsegI0mdh4mv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def sigmoid(x):\n",
        "#     return tf.keras.activations.sigmoid(x)\n",
        "\n",
        "# def tanh(x):\n",
        "#     return tf.keras.activations.tanh(x)\n",
        "\n",
        "# def relu(x):\n",
        "#     return tf.keras.activations.relu(x)\n",
        "\n",
        "# def softmax(x):\n",
        "#     return tf.keras.activations.softmax(x)"
      ],
      "metadata": {
        "id": "1jUdo75NiW8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / exp_x.sum()"
      ],
      "metadata": {
        "id": "dPzk25I1maAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-10, 10, 100)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZikTnM0ios1",
        "outputId": "f6b05489-218b-47ef-8105-5a68a569b8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n",
              "        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n",
              "        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n",
              "        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n",
              "        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n",
              "        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n",
              "        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n",
              "        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n",
              "        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n",
              "        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n",
              "        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n",
              "        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n",
              "        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n",
              "         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n",
              "         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n",
              "         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n",
              "         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n",
              "         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n",
              "         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n",
              "         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n",
              "         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n",
              "         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n",
              "         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n",
              "         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n",
              "         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_sigmoid = sigmoid(x)\n",
        "y_tanh = tanh(x)\n",
        "y_relu = relu(x)\n",
        "y_softmax = softmax(x)\n",
        "y_sigmoid, y_tanh, y_relu, y_softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQFNOl4bizB9",
        "outputId": "0971cf09-6da3-411d-c3c4-d802a1c1db68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4.53978687e-05, 5.55606489e-05, 6.79983174e-05, 8.32200197e-05,\n",
              "        1.01848815e-04, 1.24647146e-04, 1.52547986e-04, 1.86692945e-04,\n",
              "        2.28478855e-04, 2.79614739e-04, 3.42191434e-04, 4.18766684e-04,\n",
              "        5.12469082e-04, 6.27124987e-04, 7.67413430e-04, 9.39055039e-04,\n",
              "        1.14904229e-03, 1.40591988e-03, 1.72012560e-03, 2.10440443e-03,\n",
              "        2.57431039e-03, 3.14881358e-03, 3.85103236e-03, 4.70911357e-03,\n",
              "        5.75728612e-03, 7.03711536e-03, 8.59898661e-03, 1.05038445e-02,\n",
              "        1.28252101e-02, 1.56514861e-02, 1.90885420e-02, 2.32625358e-02,\n",
              "        2.83228820e-02, 3.44451957e-02, 4.18339400e-02, 5.07243606e-02,\n",
              "        6.13831074e-02, 7.41067363e-02, 8.92170603e-02, 1.07052146e-01,\n",
              "        1.27951705e-01, 1.52235823e-01, 1.80176593e-01, 2.11963334e-01,\n",
              "        2.47663801e-01, 2.87185901e-01, 3.30246430e-01, 3.76354517e-01,\n",
              "        4.24816868e-01, 4.74768924e-01, 5.25231076e-01, 5.75183132e-01,\n",
              "        6.23645483e-01, 6.69753570e-01, 7.12814099e-01, 7.52336199e-01,\n",
              "        7.88036666e-01, 8.19823407e-01, 8.47764177e-01, 8.72048295e-01,\n",
              "        8.92947854e-01, 9.10782940e-01, 9.25893264e-01, 9.38616893e-01,\n",
              "        9.49275639e-01, 9.58166060e-01, 9.65554804e-01, 9.71677118e-01,\n",
              "        9.76737464e-01, 9.80911458e-01, 9.84348514e-01, 9.87174790e-01,\n",
              "        9.89496155e-01, 9.91401013e-01, 9.92962885e-01, 9.94242714e-01,\n",
              "        9.95290886e-01, 9.96148968e-01, 9.96851186e-01, 9.97425690e-01,\n",
              "        9.97895596e-01, 9.98279874e-01, 9.98594080e-01, 9.98850958e-01,\n",
              "        9.99060945e-01, 9.99232587e-01, 9.99372875e-01, 9.99487531e-01,\n",
              "        9.99581233e-01, 9.99657809e-01, 9.99720385e-01, 9.99771521e-01,\n",
              "        9.99813307e-01, 9.99847452e-01, 9.99875353e-01, 9.99898151e-01,\n",
              "        9.99916780e-01, 9.99932002e-01, 9.99944439e-01, 9.99954602e-01]),\n",
              " array([-1.        , -0.99999999, -0.99999999, -0.99999999, -0.99999998,\n",
              "        -0.99999997, -0.99999995, -0.99999993, -0.9999999 , -0.99999984,\n",
              "        -0.99999977, -0.99999965, -0.99999947, -0.99999921, -0.99999882,\n",
              "        -0.99999823, -0.99999735, -0.99999604, -0.99999406, -0.99999111,\n",
              "        -0.99998668, -0.99998004, -0.99997011, -0.99995523, -0.99993294,\n",
              "        -0.99989955, -0.99984955, -0.99977465, -0.99966248, -0.99949449,\n",
              "        -0.9992429 , -0.99886619, -0.99830218, -0.99745797, -0.99619479,\n",
              "        -0.9943057 , -0.99148279, -0.98726936, -0.98099146, -0.97166188,\n",
              "        -0.95785067, -0.93752157, -0.90784899, -0.86506558, -0.8044548 ,\n",
              "        -0.72069563, -0.60883666, -0.46607983, -0.2940833 , -0.10066796,\n",
              "         0.10066796,  0.2940833 ,  0.46607983,  0.60883666,  0.72069563,\n",
              "         0.8044548 ,  0.86506558,  0.90784899,  0.93752157,  0.95785067,\n",
              "         0.97166188,  0.98099146,  0.98726936,  0.99148279,  0.9943057 ,\n",
              "         0.99619479,  0.99745797,  0.99830218,  0.99886619,  0.9992429 ,\n",
              "         0.99949449,  0.99966248,  0.99977465,  0.99984955,  0.99989955,\n",
              "         0.99993294,  0.99995523,  0.99997011,  0.99998004,  0.99998668,\n",
              "         0.99999111,  0.99999406,  0.99999604,  0.99999735,  0.99999823,\n",
              "         0.99999882,  0.99999921,  0.99999947,  0.99999965,  0.99999977,\n",
              "         0.99999984,  0.9999999 ,  0.99999993,  0.99999995,  0.99999997,\n",
              "         0.99999998,  0.99999999,  0.99999999,  0.99999999,  1.        ]),\n",
              " array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.1010101 ,  0.3030303 ,  0.50505051,  0.70707071,  0.90909091,\n",
              "         1.11111111,  1.31313131,  1.51515152,  1.71717172,  1.91919192,\n",
              "         2.12121212,  2.32323232,  2.52525253,  2.72727273,  2.92929293,\n",
              "         3.13131313,  3.33333333,  3.53535354,  3.73737374,  3.93939394,\n",
              "         4.14141414,  4.34343434,  4.54545455,  4.74747475,  4.94949495,\n",
              "         5.15151515,  5.35353535,  5.55555556,  5.75757576,  5.95959596,\n",
              "         6.16161616,  6.36363636,  6.56565657,  6.76767677,  6.96969697,\n",
              "         7.17171717,  7.37373737,  7.57575758,  7.77777778,  7.97979798,\n",
              "         8.18181818,  8.38383838,  8.58585859,  8.78787879,  8.98989899,\n",
              "         9.19191919,  9.39393939,  9.5959596 ,  9.7979798 , 10.        ]),\n",
              " array([3.77029476e-10, 4.61436094e-10, 5.64739054e-10, 6.91168729e-10,\n",
              "        8.45902561e-10, 1.03527708e-09, 1.26704739e-09, 1.55070475e-09,\n",
              "        1.89786526e-09, 2.32274554e-09, 2.84274493e-09, 3.47915801e-09,\n",
              "        4.25804662e-09, 5.21130715e-09, 6.37797672e-09, 7.80583179e-09,\n",
              "        9.55334469e-09, 1.16920781e-08, 1.43096155e-08, 1.75131483e-08,\n",
              "        2.14338646e-08, 2.62323225e-08, 3.21050242e-08, 3.92924637e-08,\n",
              "        4.80889749e-08, 5.88547851e-08, 7.20307667e-08, 8.81564912e-08,\n",
              "        1.07892326e-07, 1.32046476e-07, 1.61608081e-07, 1.97787724e-07,\n",
              "        2.42067002e-07, 2.96259203e-07, 3.62583560e-07, 4.43756133e-07,\n",
              "        5.43101031e-07, 6.64686542e-07, 8.13491735e-07, 9.95610352e-07,\n",
              "        1.21850036e-06, 1.49128937e-06, 1.82514839e-06, 2.23374935e-06,\n",
              "        2.73382492e-06, 3.34585378e-06, 4.09489921e-06, 5.01163548e-06,\n",
              "        6.13360401e-06, 7.50675071e-06, 9.18730750e-06, 1.12440951e-05,\n",
              "        1.37613414e-05, 1.68421305e-05, 2.06126243e-05, 2.52272289e-05,\n",
              "        3.08749175e-05, 3.77869697e-05, 4.62464418e-05, 5.65997592e-05,\n",
              "        6.92709019e-05, 8.47787680e-05, 1.03758422e-04, 1.26987104e-04,\n",
              "        1.55416054e-04, 1.90209471e-04, 2.32792185e-04, 2.84908007e-04,\n",
              "        3.48691141e-04, 4.26753580e-04, 5.22292070e-04, 6.39219023e-04,\n",
              "        7.82322732e-04, 9.57463460e-04, 1.17181342e-03, 1.43415049e-03,\n",
              "        1.75521768e-03, 2.14816306e-03, 2.62907820e-03, 3.21765712e-03,\n",
              "        3.93800281e-03, 4.81961426e-03, 5.89859447e-03, 7.21912894e-03,\n",
              "        8.83529506e-03, 1.08132767e-02, 1.32340745e-02, 1.61968229e-02,\n",
              "        1.98228499e-02, 2.42606455e-02, 2.96919425e-02, 3.63391588e-02,\n",
              "        4.44745056e-02, 5.44311347e-02, 6.66167816e-02, 8.15304625e-02,\n",
              "        9.97829100e-02, 1.22121583e-01, 1.49461275e-01, 1.82921579e-01]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot the results\n",
        "# plt.figure(figsize=(12, 8))\n",
        "\n",
        "# plt.subplot(2, 2, 1)\n",
        "# plt.plot(x, y_sigmoid)\n",
        "# plt.title('Sigmoid Activation Function')\n",
        "\n",
        "# plt.subplot(2, 2, 2)\n",
        "# plt.plot(x, y_tanh)\n",
        "# plt.title('Tanh Activation Function')\n",
        "\n",
        "# plt.subplot(2, 2, 3)\n",
        "# plt.plot(x, y_relu)\n",
        "# plt.title('ReLU Activation Function')\n",
        "\n",
        "# plt.subplot(2, 2, 4)\n",
        "# plt.plot(x, y_softmax)\n",
        "# plt.title('Softmax Activation Function')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "bbjn8ELIjAdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "\n",
        "\n",
        "# print(tf.nn.tanh(predictions).numpy())\n",
        "\n",
        "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "print(model.evaluate(x_test,  y_test, verbose=2))\n",
        "\n",
        "# probability_model = tf.keras.Sequential([\n",
        "#   model,\n",
        "#   tf.keras.layers.Softmax()\n",
        "# ])\n",
        "# print(probability_model(x_test[:5]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-fbZ1RGwlq0",
        "outputId": "0e8c4211-0842-4fc7-d76f-2e44875e3170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7842 - loss: 0.6523\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8076 - loss: 0.6031\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8274 - loss: 0.5412\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8387 - loss: 0.5103\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8508 - loss: 0.4706\n",
            "313/313 - 1s - 2ms/step - accuracy: 0.9387 - loss: 0.2327\n",
            "[0.23273546993732452, 0.9387000203132629]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='tanh'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "print(\"Predictions: \", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCt5T0J7xJQk",
        "outputId": "b0ef51cc-a2c4-49b0-b8f1-90e3603512e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:  [[-0.09900242  0.46363515 -0.3012781   0.23611489 -0.45185992  0.6452898\n",
            "  -0.9504489  -0.3791827   0.8060753   0.4489795 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "print(\"Predictions: \", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m63ebF4CxM2v",
        "outputId": "ddfcd5f8-208d-4049-8c48-6d0e26da6011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:  [[ 0.1691619  -0.0273245   0.10333499 -0.4655018  -0.20277794 -0.03630955\n",
            "   0.7308886  -0.08672032  0.21913233 -0.20052204]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='sigmoid'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "print(\"Predictions: \", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsTj7XPexOEY",
        "outputId": "328553f6-1684-46dc-fef4-f2fe7e84a1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:  [[-0.28386262 -0.79435563 -0.5966868   1.4479383   1.0309739  -0.24247704\n",
            "   0.162836   -0.5787261  -0.5570216   0.8040716 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='softmax'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "print(\"Predictions: \", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btirzfsgxPLg",
        "outputId": "81b38ab5-8e80-4f0f-fbcb-9f4272a79695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:  [[ 0.00417044 -0.01521724 -0.00610546  0.0047087  -0.02625129  0.00423488\n",
            "   0.00844995  0.0061378   0.00619716 -0.0174382 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Taking input from an existing dataset\n"
      ],
      "metadata": {
        "id": "LyUdKd3xtE00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Activation Functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # for numerical stability\n",
        "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Display the first few samples of the dataset\n",
        "print(\"Input Values (first 5 samples):\")\n",
        "print(pd.DataFrame(X, columns=iris.feature_names).head())\n",
        "\n",
        "# Calculate and display outputs for the first sample\n",
        "sample = X[:5]  # Taking first 5 samples for demonstration\n",
        "\n",
        "print(\"\\nSigmoid Output:\")\n",
        "print(sigmoid(sample))\n",
        "\n",
        "print(\"\\nTanh Output:\")\n",
        "print(tanh(sample))\n",
        "\n",
        "print(\"\\nReLU Output:\")\n",
        "print(relu(sample))\n",
        "\n",
        "# Softmax expects a 2D array, using the first few samples\n",
        "print(\"\\nSoftmax Output:\")\n",
        "print(softmax(sample))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh13SX0rsH66",
        "outputId": "525fd392-774e-4631-ca5c-70c2250ebf66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Values (first 5 samples):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            "3                4.6               3.1                1.5               0.2\n",
            "4                5.0               3.6                1.4               0.2\n",
            "\n",
            "Sigmoid Output:\n",
            "[[0.9939402  0.97068777 0.80218389 0.549834  ]\n",
            " [0.99260846 0.95257413 0.80218389 0.549834  ]\n",
            " [0.9909867  0.96083428 0.78583498 0.549834  ]\n",
            " [0.9900482  0.95689275 0.81757448 0.549834  ]\n",
            " [0.99330715 0.97340301 0.80218389 0.549834  ]]\n",
            "\n",
            "Tanh Output:\n",
            "[[0.99992566 0.9981779  0.88535165 0.19737532]\n",
            " [0.9998891  0.99505475 0.88535165 0.19737532]\n",
            " [0.99983457 0.9966824  0.86172316 0.19737532]\n",
            " [0.99979794 0.99594936 0.90514825 0.19737532]\n",
            " [0.9999092  0.99850794 0.88535165 0.19737532]]\n",
            "\n",
            "ReLU Output:\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "\n",
            "Softmax Output:\n",
            "[[0.81032902 0.16360261 0.02003419 0.00603418]\n",
            " [0.84114103 0.1258083  0.02540026 0.00765041]\n",
            " [0.78888466 0.17602396 0.02632766 0.00876372]\n",
            " [0.78097135 0.17425826 0.03518214 0.00958825]\n",
            " [0.77993968 0.19233076 0.02131086 0.00641871]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P3\n"
      ],
      "metadata": {
        "id": "465_fzbNzmZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.output_size = output_size\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Initialize layer sizes\n",
        "        layer_sizes = [input_size] + hidden_layers + [output_size]\n",
        "\n",
        "        # Xavier initialization for weights and biases\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
        "            weight = np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i + 1]))\n",
        "            bias = np.zeros((1, layer_sizes[i + 1]))\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(bias)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        self.a = [X]\n",
        "        for weight, bias in zip(self.weights, self.biases):\n",
        "            z = self.a[-1].dot(weight) + bias\n",
        "            a = self.sigmoid(z)\n",
        "            self.a.append(a)\n",
        "        return self.a[-1]\n",
        "\n",
        "    def backpropagate(self, X, y, learning_rate):\n",
        "        output = self.feedforward(X)\n",
        "        output_error = y - output\n",
        "        deltas = [output_error * self.sigmoid_derivative(output)]\n",
        "\n",
        "        # Backpropagation\n",
        "        for i in reversed(range(len(self.weights) - 1)):\n",
        "            error = deltas[-1].dot(self.weights[i + 1].T)\n",
        "            delta = error * self.sigmoid_derivative(self.a[i + 1])\n",
        "            deltas.append(delta)\n",
        "\n",
        "        deltas.reverse()\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] += self.a[i].T.dot(deltas[i]) * learning_rate\n",
        "            self.biases[i] += np.sum(deltas[i], axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            self.backpropagate(X, y, learning_rate)\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(y - self.feedforward(X)))\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = load_iris()\n",
        "    X = data.data\n",
        "    y = data.target.reshape(-1, 1)\n",
        "\n",
        "    # One-hot encoding the labels\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    y = encoder.fit_transform(y)\n",
        "\n",
        "    # Create and train the neural network\n",
        "    nn = NeuralNetwork(input_size=4, hidden_layers=[10, 10, 10, 10], output_size=3)\n",
        "    nn.train(X, y, epochs=10000, learning_rate=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKuUi3FbwbpE",
        "outputId": "87b8424e-60f4-425f-9728-3bd565c891ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2732390192573928\n",
            "Epoch 1000, Loss: 0.12326484948717387\n",
            "Epoch 2000, Loss: 0.012460673982972188\n",
            "Epoch 3000, Loss: 0.015928069988262298\n",
            "Epoch 4000, Loss: 0.01209105861454704\n",
            "Epoch 5000, Loss: 0.008140564886561999\n",
            "Epoch 6000, Loss: 0.0115295175526929\n",
            "Epoch 7000, Loss: 0.0048605017647235975\n",
            "Epoch 8000, Loss: 0.004592763595337956\n",
            "Epoch 9000, Loss: 0.004523608647455111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights = [np.random.uniform(-1, 1, (input_size, hidden_size)),\n",
        "                        np.random.uniform(-1, 1, (hidden_size, output_size))]\n",
        "        self.biases = [np.zeros((1, hidden_size)), np.zeros((1, output_size))]\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def train(self, X, y, epochs, lr):\n",
        "        for _ in range(epochs):\n",
        "            # Forward pass\n",
        "            activations = [X]  # Store activations of each layer\n",
        "            a = X\n",
        "            for w, b in zip(self.weights, self.biases):\n",
        "                z = a.dot(w) + b\n",
        "                a = self.sigmoid(z)\n",
        "                activations.append(a)\n",
        "\n",
        "            # Backpropagation\n",
        "            error = y - activations[-1]  # Error at the output layer\n",
        "\n",
        "            for i in reversed(range(len(self.weights))):\n",
        "                # Calculate delta for current layer\n",
        "                if i == len(self.weights) - 1:  # Output layer\n",
        "                    delta = error * activations[-1] * (1 - activations[-1])\n",
        "                else:  # Hidden layer\n",
        "                    delta = error.dot(self.weights[i + 1].T) * activations[i + 1] * (1 - activations[i + 1])\n",
        "\n",
        "                # Update weights and biases for current layer\n",
        "                self.weights[i] += activations[i].T.dot(delta) * lr  # Use activations of previous layer for weight update\n",
        "                self.biases[i] += np.sum(delta, axis=0, keepdims=True) * lr\n",
        "\n",
        "                # Update error for the next layer (previous in the reversed loop)\n",
        "                error = delta\n",
        "\n"
      ],
      "metadata": {
        "id": "YD7ahs65yXHK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def train(self, X, y, epochs, lr):\n",
        "        for _ in range(epochs):\n",
        "            z1 = X.dot(self.W1) + self.b1\n",
        "            a1 = self.sigmoid(z1)\n",
        "            z2 = a1.dot(self.W2) + self.b2\n",
        "            a2 = self.sigmoid(z2)\n",
        "            error = y - a2\n",
        "            self.W2 += a1.T.dot(error * a2 * (1 - a2)) * lr\n",
        "            self.b2 += np.sum(error * a2 * (1 - a2), axis=0, keepdims=True) * lr\n",
        "            self.W1 += X.T.dot((error * a2 * (1 - a2)).dot(self.W2.T) * a1 * (1 - a1)) * lr\n",
        "            self.b1 += np.sum((error * a2 * (1 - a2)).dot(self.W2.T) * a1 * (1 - a1), axis=0, keepdims=True) * lr\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = load_iris()\n",
        "    X = data.data\n",
        "    y = OneHotEncoder(sparse_output=False).fit_transform(data.target.reshape(-1, 1))\n",
        "    nn = SimpleNN(input_size=4, hidden_size=3, output_size=3)\n",
        "    nn.train(X, y, epochs=1000, lr=0.01)\n"
      ],
      "metadata": {
        "id": "qiEpTk1OzDXN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def train(self, X, y, epochs, lr):\n",
        "        for _ in range(epochs):\n",
        "            z1 = X.dot(self.W1) + self.b1\n",
        "            a1 = self.sigmoid(z1)\n",
        "            z2 = a1.dot(self.W2) + self.b2\n",
        "            a2 = self.sigmoid(z2)\n",
        "            error = y - a2\n",
        "            self.W2 += a1.T.dot(error * a2 * (1 - a2)) * lr\n",
        "            self.b2 += np.sum(error * a2 * (1 - a2), axis=0, keepdims=True) * lr\n",
        "            self.W1 += X.T.dot((error * a2 * (1 - a2)).dot(self.W2.T) * a1 * (1 - a1)) * lr\n",
        "            self.b1 += np.sum((error * a2 * (1 - a2)).dot(self.W2.T) * a1 * (1 - a1), axis=0, keepdims=True) * lr\n",
        "\n",
        "        # Print final predictions\n",
        "        final_predictions = self.sigmoid(a2)\n",
        "        print(\"Final Predictions (first 5):\", final_predictions[:5])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = load_iris()\n",
        "    X = data.data\n",
        "    y = OneHotEncoder(sparse_output=False).fit_transform(data.target.reshape(-1, 1))\n",
        "    nn = SimpleNN(input_size=4, hidden_size=3, output_size=3)\n",
        "    nn.train(X, y, epochs=1000, lr=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "851uynxezUOt",
        "outputId": "70e2bbc8-1102-463e-8793-5f46dfc10c83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Predictions (first 5): [[0.58256462 0.58257126 0.58257155]\n",
            " [0.58256367 0.58257717 0.58256906]\n",
            " [0.58255778 0.58259083 0.5825667 ]\n",
            " [0.58255763 0.58259469 0.58256537]\n",
            " [0.5825628  0.58257557 0.58257073]]\n"
          ]
        }
      ]
    }
  ]
}